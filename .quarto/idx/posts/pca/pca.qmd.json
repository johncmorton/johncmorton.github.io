{"title":"A county like mine","markdown":{"yaml":{"title":"A county like mine","categories":["politics","visualization","statistics","python"],"image":"pca.png","jupyter":"python3","format":{"html":{"code-fold":true}}},"headingText":"Import","containsRefs":false,"markdown":"\n\nPrincipal Component Analysis (PCA) is a statistical technique used to simplify complex data by reducing the number of variables while retaining most of the original information. In other words, it helps to identify the most important patterns or relationships in large datasets by transforming them into a set of linearly uncorrelated variables, known as principal components.\n\nPCA works by identifying the underlying structure of the data and extracting the directions of maximum variance, which are the principal components. Each principal component is a linear combination of the original variables, and they are orthogonal to each other, meaning that they are uncorrelated. The first principal component captures the most significant variation in the data, and subsequent components capture decreasing amounts of variation. By selecting the appropriate number of principal components, we can reduce the dimensionality of the dataset while retaining the essential information.\n\nIn this data science project, I used Principal Component Analysis (PCA) to visualize and explore the similarity of various counties in the United States based on a set of demographic metrics including th following:\n\n-   Population estimates\n-   Poverty estimates\n-   Unemployment rates\n-   Crime rates\n-   Racial demographics\n-   Political leanings\n-   Job industry composition\n-   General health status\n-   Social capital\n\nThe primary utility of using PCA in this context is to reduce the dimensionality of the dataset while retaining as much information as possible. By doing so, we can efficiently examine patterns and relationships among counties in a lower-dimensional space.\n\nFor each of these data, I applied PCA to transform the original high-dimensional data into a lower-dimensional space that still captures most of the variation present in the original dataset.\n\nBy plotting the first two or three principal components on a 2D scatter plot, we can visualize the relationships among counties based on their transformed coordinates. This allows us to identify clusters of similar counties, outliers, or any other interesting patterns. In the code, I also highlighted a specific county (represented by highlighted_area), making it easier to identify its position relative to other counties in the reduced-dimensional space.\n\nAdditionally, I created a bar chart displaying the proportion of variance explained by each principal component, which helps to determine the optimal number of components to retain for further analysis or modeling.\n\nThe use of PCA in this project allows us to effectively summarize the complex, high-dimensional relationships between counties, making it easier to identify patterns and interpret the data.\n\n```{python}\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\nimport plotly.graph_objects as go\n```\n\n# Population\n\n```{python}\n# Population Estimates\npop = pd.read_excel(\"PopulationEstimates.xlsx\", skiprows=4)\n\npop.columns = ['fips_code', 'state', 'area_name', 'rucc_2013', 'pop_1990', 'pop_2000', 'pop_2010', 'pop_2020', 'pop_2021']\n\npop = (pop\n    .filter(['fips_code', 'state', 'area_name', 'rucc_2013','pop_2021'], axis=1)\n    .query(\"state != 'Puerto Rico'\"))\n\npol = pd.read_csv('politics.csv').rename({'fips':'fips_code'}, axis=1)\n\npop.head()\n```\n\n```{python}\ndef make_pca(new_df, highlighted_area, dim=2):\n    # Merge\n    df_raw = (pd\n        .merge(pop, new_df, on='fips_code', how='left')\n        .dropna()\n        .query(\"state != 'Puerto Rico'\"))\n\n    # Normalize data\n    scaler = StandardScaler()\n    df = scaler.fit_transform(df_raw.drop(['fips_code', 'state', 'area_name'], axis=1))\n    df_name = df_raw[['fips_code', 'state', 'area_name']].reset_index(drop=True)\n\n    # PCA analysis\n    pca = PCA()\n    pca.fit(df)\n    pcs = pca.transform(df)\n    pcs_df = pd.DataFrame(pcs)\n    df_final = pd.concat([df_name, pcs_df], axis=1)\n\n    df_return = df_final.drop(['state', 'area_name'], axis=1)\n\n    fig2 = pca_variance(pca)\n\n    # Marks\n    marker_colors = df_final['fips_code'].apply(lambda x: 'red' if x == highlighted_area else 'blue')\n    marker_sizes = df_final['fips_code'].apply(lambda x: 15 if x == highlighted_area else 5)\n\n    if dim == 2:\n        # Chart PC distance 2D\n        fig = px.scatter(df_final,\n            x=df_final.columns[len(df_name.columns)],\n            y=df_final.columns[len(df_name.columns) + 1],\n            hover_data=['fips_code','state','area_name'],\n            custom_data=['fips_code'],\n            color=marker_colors,\n            size=marker_sizes,\n            color_discrete_map={'red': 'red', 'blue': 'blue'})\n\n        fig.update_traces(marker=dict(symbol='circle', line=dict(width=1))\n        )\n\n        return fig, fig2, df_return\n\n    elif dim == 3:\n        fig = px.scatter_3d(df_final,\n            x=df_final.columns[len(df_name.columns)],\n            y=df_final.columns[len(df_name.columns) + 1],\n            z=df_final.columns[len(df_name.columns) + 2],\n            hover_data=['fips_code','state','area_name'],\n            custom_data=['fips_code'],\n            color=marker_colors,\n            size=marker_sizes,\n            color_discrete_map={'red': 'red', 'blue': 'blue'})\n\n        fig.update_traces(marker=dict(symbol='circle', line=dict(width=1)))\n\n        return fig, fig2, df_return\n\ndef pca_variance(pca):\n    # Explained variance\n    explained_variance_ratio = pca.explained_variance_ratio_\n    num_components = len(explained_variance_ratio)\n    components = list(range(1, num_components + 1))\n\n    fig = go.Figure(data=[go.Bar(x=components, y=explained_variance_ratio)])\n\n    fig.update_layout(\n        title=\"Proportion of Variance Explained by Principal Components\",\n        xaxis_title=\"Principal Components\",\n        yaxis_title=\"Explained Variance Ratio\",\n        yaxis=dict(tickformat=\".2%\"),\n    )\n\n    return fig\n\n```\n\n# Poverty\n\nPoverty variables:\n\n-   percent of population below poverty level overall in county 2020 (pct_povall_2020)\n-   percent of population below poverty level for minors in county 2020 (pct_pov017_2020)\n-   median household income in county 2020 (medhhinc_2020)\n\n```{python}\n# Poverty Estimates\npov = pd.read_excel(\"PovertyEstimates.xlsx\", skiprows=4)\n\npov.columns = ['fips_code', 'stabr', 'area_name', 'rucc_2003', 'uic_2003', 'rucc_2013', 'uic_2013', 'povall_2020', 'ci90lb_all_2020', 'ci90ub_all_2020', 'pct_povall_2020', 'ci90lb_allp_2020', 'ci90ub_allp_2020', 'pov017_2020', 'ci90lb_017_2020', 'ci90ub_017_2020', 'pct_pov017_2020', 'ci90lb_017p_2020', 'ci90ub_017p_2020', 'pov517_2020', 'ci90lb_517_2020', 'ci90ub_517_2020', 'pct_pov517_2020', 'ci90lb_517p_2020', 'ci90ub_517p_2020', 'medhhinc_2020', 'ci90lb_inc_2020', 'ci90ub_inc_2020', 'pov04_2020', 'ci90lb_04_2020', 'ci90ub_04_2020', 'pct_pov04_2020', 'ci90lb_04p_2020', 'ci90ub_04p_2020']\n\npov_filtered_columns = ['fips_code','pct_povall_2020','pct_pov017_2020','medhhinc_2020']\n\npov = pov[pov_filtered_columns]\n```\n\n# Education\n\nEducation variables:\n\n-   percent of population with less than high school diploma in county 2017-2021 (pct_less_than_hs_2017_21)\n-   percent of population with high school diploma in county 2017-2021 (pct_hs_diploma_2017_21)\n-   percent of population with some college or associate degree in county 2017-2021 (pct_some_college_assoc_degree_2017_21)\n-   percent of population with bachelor's degree or higher in county 2017-2021 (pct_bachelors_plus_2017_21)\n\n```{python}\n# Education\nedu = pd.read_excel(\"Education.xlsx\", skiprows=3)\n\nedu.columns = ['fips_code', 'state', 'area_name', 'rucc_2003', 'uic_2003', 'rucc_2013', 'uic_2013', 'less_than_hs_1970', 'hs_diploma_1970', 'some_college_1_3_yrs_1970', 'college_4_yrs_plus_1970', 'pct_less_than_hs_1970', 'pct_hs_diploma_1970', 'pct_some_college_1_3_yrs_1970', 'pct_college_4_yrs_plus_1970', 'less_than_hs_1980', 'hs_diploma_1980', 'some_college_1_3_yrs_1980', 'college_4_yrs_plus_1980', 'pct_less_than_hs_1980', 'pct_hs_diploma_1980', 'pct_some_college_1_3_yrs_1980', 'pct_college_4_yrs_plus_1980', 'less_than_hs_1990', 'hs_diploma_1990', 'some_college_assoc_degree_1990', 'bachelors_plus_1990', 'pct_less_than_hs_1990', 'pct_hs_diploma_1990', 'pct_some_college_assoc_degree_1990', 'pct_bachelors_plus_1990', 'less_than_hs_2000', 'hs_diploma_2000', 'some_college_assoc_degree_2000', 'bachelors_plus_2000', 'pct_less_than_hs_2000', 'pct_hs_diploma_2000', 'pct_some_college_assoc_degree_2000', 'pct_bachelors_plus_2000', 'less_than_hs_2008_12', 'hs_diploma_2008_12', 'some_college_assoc_degree_2008_12', 'bachelors_plus_2008_12', 'pct_less_than_hs_2008_12', 'pct_hs_diploma_2008_12', 'pct_some_college_assoc_degree_2008_12', 'pct_bachelors_plus_2008_12', 'less_than_hs_2017_21', 'hs_diploma_2017_21', 'some_college_assoc_degree_2017_21', 'bachelors_plus_2017_21', 'pct_less_than_hs_2017_21', 'pct_hs_diploma_2017_21', 'pct_some_college_assoc_degree_2017_21', 'pct_bachelors_plus_2017_21']\n\nedu_filtered_columns = [col for col in edu.columns if ('2017' in col and 'pct' in col) or 'fips' in col.lower()]\n\nedu = edu[edu_filtered_columns]\n```\n\n# Unemployment\n\nUnemployment variables:\n\n-   Civilian labor force in county 2021 (civ_labor_force_2021)\n-   Employed in county 2021 (employed_2021)\n-   Unemployed in county 2021 (unemployed_2021)\n-   Unemployment rate in county 2021 (unemp_rate_2021)\n-   Gini coefficient in county 2019? (Gini.Coefficient)\n\n```{python}\n# Unemployment\nunemp = pd.read_excel(\"Unemployment.xlsx\", skiprows=4)\n\nunemp.columns = ['fips_code', 'state', 'area_name', 'rucc_2013', 'uic_2013', 'metro_2013', 'civ_labor_force_2000', 'employed_2000', 'unemployed_2000', 'unemp_rate_2000', 'civ_labor_force_2001', 'employed_2001', 'unemployed_2001', 'unemp_rate_2001', 'civ_labor_force_2002', 'employed_2002', 'unemployed_2002', 'unemp_rate_2002', 'civ_labor_force_2003', 'employed_2003', 'unemployed_2003', 'unemp_rate_2003', 'civ_labor_force_2004', 'employed_2004', 'unemployed_2004', 'unemp_rate_2004', 'civ_labor_force_2005', 'employed_2005', 'unemployed_2005', 'unemp_rate_2005', 'civ_labor_force_2006', 'employed_2006', 'unemployed_2006', 'unemp_rate_2006', 'civ_labor_force_2007', 'employed_2007', 'unemployed_2007', 'unemp_rate_2007', 'civ_labor_force_2008', 'employed_2008', 'unemployed_2008', 'unemp_rate_2008', 'civ_labor_force_2009', 'employed_2009', 'unemployed_2009', 'unemp_rate_2009', 'civ_labor_force_2010', 'employed_2010', 'unemployed_2010', 'unemp_rate_2010', 'civ_labor_force_2011', 'employed_2011', 'unemployed_2011', 'unemp_rate_2011', 'civ_labor_force_2012', 'employed_2012', 'unemployed_2012', 'unemp_rate_2012', 'civ_labor_force_2013', 'employed_2013', 'unemployed_2013', 'unemp_rate_2013', 'civ_labor_force_2014', 'employed_2014', 'unemployed_2014', 'unemp_rate_2014', 'civ_labor_force_2015', 'employed_2015', 'unemployed_2015', 'unemp_rate_2015', 'civ_labor_force_2016', 'employed_2016', 'unemployed_2016', 'unemp_rate_2016', 'civ_labor_force_2017', 'employed_2017', 'unemployed_2017', 'unemp_rate_2017', 'civ_labor_force_2018', 'employed_2018', 'unemployed_2018', 'unemp_rate_2018', 'civ_labor_force_2019', 'employed_2019', 'unemployed_2019', 'unemp_rate_2019', 'civ_labor_force_2020', 'employed_2020', 'unemployed_2020', 'unemp_rate_2020', 'civ_labor_force_2021', 'employed_2021', 'unemployed_2021', 'unemp_rate_2021', 'med_hh_income_2020', 'med_hh_income_pct_state_total_2020']\n\nunemp_filtered_columns = [col for col in unemp.columns if '2021' in col or 'fips' in col.lower()]\n\nunemp = unemp[unemp_filtered_columns]\n\nunemp_cols = ['civ_labor_force_2021','employed_2021','unemployed_2021']\n\nunemp2 = pd.merge(unemp, pop.filter(['fips_code','pop_2021']), how = 'left',\n     left_on='fips_code', right_on='fips_code'\n)\n\nunemp[unemp_cols] = (unemp2[unemp_cols].div(unemp2['pop_2021'], axis=0))*100\n\nunemp = pd.merge(unemp, pol[['fips_code','Gini.Coefficient']], how = 'left',\n     left_on='fips_code', right_on='fips_code'\n)\n```\n\n# Crime\n\nCrime Variables\n\n-   Crime rate/100k people\n-   Murders/capita\n-   Rape/capita\n-   Robbery/capita\n-   Aggravated assault/capita\n-   Burglaries/capita\n-   Larcenies/capita\n-   Motor vehicle thefts/capita\n-   Arsons/capita\n\n```{python}\n# Crime\ncrime = pd.read_csv(\"crime_clean.csv\").drop('Unnamed: 0', axis=1)\n\ncrime.columns = ['fips_code','crime_rate_per_100k','murder','rape','robbery','ag_assault','burglry','larceny','mv_theft','arson','population']\n\ncrime_cols = ['murder','rape','robbery','ag_assault','burglry','larceny','mv_theft','arson']\n\ncrime[crime_cols] = crime[crime_cols].div(crime['population'], axis=0)*100000\n\ncrime = crime.drop('population', axis=1)\na, b, crime_df = make_pca(crime, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Race\n\nRace Variables\n\n-   White (%)\n-   Black (%)\n-   American Indian/Alaska Native (%)\n-   Asian (%)\n-   Native Hawaiian/Pacific Islander (%)\n-   Other race (%)\n-   Two or more races (%)\n-   Hispanic/Latino (%)\n\n```{python}\n# Race\nrace = pd.read_csv('race.csv')\n\nrace_cols = ['white', 'black','amerindian', 'asian', 'hawaiian_or_PI', 'other_race', 'two_or_more','hispanic']\n\nrace[race_cols] = race.filter(race_cols, axis=1).div(race['total_population'],axis=0)*100\n\nrace = race.drop(['county_name','total_population'], axis=1)\na, b, race_df = make_pca(race, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Politics\n\nPolitical Variables\n\n-   Republican Vote Share in 2016 Presidential Election (%)\n-   Democratic Vote Share in 2016 Presidential Election (%)\n-   Libertarian Vote Share in 2016 Presidential Election (%)\n\n```{python}\n# Politics\npolitics = ['fips_code','rep16_frac',\n'dem16_frac',\n'libert16_frac']\npolitics = pol[politics].fillna(0)\n```\n\n# Job Industry\n\nJob Industry Variables\n\n-   Management, professional, and related occupations (%)\n-   Service occupations (%)\n-   Sales and office occupations (%)\n-   Farming, fishing, and forestry occupations (%)\n-   Construction, extraction, maintenance, and repair occupations (%)\n-   Production, transportation, and material moving occupations (%)\n\n```{python}\n# Job Industry\njobs = ['fips_code','Management.professional.and.related.occupations',\n'Service.occupations',\n'Sales.and.office.occupations',\n'Farming.fishing.and.forestry.occupations',\n'Construction.extraction.maintenance.and.repair.occupations',\n'Production.transportation.and.material.moving.occupations']\n\njobs = pol[jobs].fillna(0)\n\njobs.columns = ['fips_code', 'mgmt', 'service', 'sales', 'farming', 'construction', 'production']\n\njobs2 = pd.merge(jobs, pop.filter(['fips_code','pop_2021'], axis=1), how = 'left', left_on='fips_code', right_on='fips_code')\n\njobs_cols = ['mgmt', 'service', 'sales', 'farming', 'construction', 'production']\n\njobs[jobs_cols] = ((jobs2[jobs_cols].div(jobs2['pop_2021'], axis=0))*100000).fillna(0)\na, b, jobs_df= make_pca(jobs, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Health\n\nHealth Variables\n\n-   Poor physical health days (%)\n-   Poor mental health days (%)\n-   Low birthweight (%)\n-   Teen births (%)\n-   Adult smoking (%)\n-   Adult obesity (%)\n-   Diabetes (%)\n-   Sexually transmitted infections (%)\n\n```{python}\n# Health\nhealth = ['fips_code','Poor.physical.health.days', 'Poor.mental.health.days', 'Low.birthweight', 'Teen.births', 'Adult.smoking', 'Adult.obesity', 'Diabetes','Sexually.transmitted.infections']\n\nhealth = pol[health].fillna(0)\n\nhealth.columns = ['fips_code', 'poor_phys', 'poor_mental', 'low_birth_lbs', 'teen_births', 'smokers','obesity','diabetes', 'stds']\n\nhealth2 = pd.merge(health, pop.filter(['fips_code','pop_2021'], axis=1), how = 'left', left_on='fips_code', right_on='fips_code')\n\nhealth_cols = ['poor_phys', 'poor_mental', 'low_birth_lbs', 'teen_births', 'smokers','obesity','diabetes', 'stds']\n\nhealth[health_cols] = ((health[health_cols].div(jobs2['pop_2021'], axis=0))*100000).fillna(0)\na, b, health_df = make_pca(health, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Social Capital\n\n-   Percentage of births to unmarried mothers (%)\n-   Percentage of women who are married (%)\n-   Percentage of children in single-parent households (%)\n-   Non-religious non-profit organizations per 1,000 people\n-   Religious congregations per 1,000 people\n-   Informal civic engagement sub-index\n-   Presidential election voting rate 2012-2016\n-   Mail-back census response rate\n-   Confidence in institutions sub-index\n-   Membership organizations per 1,000 people\n-   Recreation and leisure establishments per 1,000 people\n-   Associations per 1,000 people (Penn State method)\n-   Non-religious and religious organizations per 1,000 people\n-   Religious adherents per 1,000 people\n-   Percentage of people who receive emotional support sometimes, rarely, or never\n-   Share of middle-class itemizers deducting charitable contributions (%)\n-   Ratio of 80th to 20th percentile household income\\*\n\n```{python}\n# Social Capital\nsoccap = pd.read_csv('soccap.csv')\n\nsoccap.columns=['fips_code', 'pct_births_unmarried', 'pct_women_married', 'pct_children_single_parent', 'non_religious_non_profit_orgs_per_1k',\n'religious_congregations_per_1k', 'informal_civic_engagement_subidx', 'presidential_election_voting_rate_12_16',\n'mail_back_census_response_rate', 'confidence_in_institutions_subidx', 'membership_orgs_per_1k', 'recreation_leisure_est_per_1k',\n'associations_per_1k_penn_state_method', 'non_religious_and_religious_orgs_per_1k', 'religious_adherents_per_1k',\n'pct_emotional_support_sometimes_rarely_or_never', 'charitable_contributions_share_of_agi_middle_class_itemizers',\n'share_of_middle_class_itemizers_deducting_charitable_contributions', 'ratio_80th_to_20th_pct_hh_income']\na, b, soccap_df = make_pca(soccap, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# PCA with all the data\n\n```{python}\ndf_raw = pop.drop(['rucc_2013'], axis = 1).pipe(\n    pd.merge, pov, on='fips_code').pipe(\n    pd.merge, edu, on='fips_code').pipe(\n    pd.merge, unemp, on='fips_code').pipe(\n    pd.merge, crime, on='fips_code').pipe(\n    pd.merge, race, on='fips_code').pipe(\n    pd.merge, politics, on='fips_code').pipe(\n    pd.merge, jobs, on='fips_code').pipe(\n    pd.merge, health, on='fips_code').pipe(\n    pd.merge, soccap, on='fips_code')\n\nfor col in df_raw.drop(['fips_code','state','area_name'],axis=1).columns:\n    df_raw[col] = df_raw[col].fillna(df_raw[col].mean())\n\nhighlighted_area = 48209\n\n# Normalize data\nscaler = StandardScaler()\ndf = scaler.fit_transform(df_raw.drop(['fips_code', 'state', 'area_name'], axis=1))\ndf_name = df_raw[['fips_code', 'state', 'area_name']].reset_index(drop=True)\n\n# PCA analysis\npca = PCA()\npca.fit(df)\npcs = pca.transform(df)\npcs_df = pd.DataFrame(pcs)\ndf_final = pd.concat([df_name, pcs_df], axis=1)\n\nfig2 = pca_variance(pca)\n\n# Marks\nmarker_colors = df_final['fips_code'].apply(lambda x: 'red' if x == highlighted_area else 'blue')\nmarker_sizes = df_final['fips_code'].apply(lambda x: 15 if x == highlighted_area else 5)\n\n\n# Chart PC distance 2D\nfig = px.scatter(df_final,\n    x=df_final.columns[len(df_name.columns)],\n    y=df_final.columns[len(df_name.columns) + 1],\n    hover_data=['fips_code','state','area_name'],\n    custom_data=['fips_code'],\n    color=marker_colors,\n    size=marker_sizes,\n    color_discrete_map={'red': 'red', 'blue': 'blue'})\n\nfig.update_traces(marker=dict(symbol='circle', line=dict(width=1))\n)\n\nfig.show()\n```\n\n```{python}\nfig = px.scatter_3d(df_final,\n    x=df_final.columns[len(df_name.columns)],\n    y=df_final.columns[len(df_name.columns) + 1],\n    z=df_final.columns[len(df_name.columns) + 2],\n    hover_data=['fips_code','state','area_name'],\n    custom_data=['fips_code'],\n    color=marker_colors,\n    size=marker_sizes,\n    color_discrete_map={'red': 'red', 'blue': 'blue'})\n\nfig.update_traces(marker=dict(symbol='circle', line=dict(width=1)))\n\nfig.show()\n```\n\n```{python}\n# Explained variance\nexplained_variance_ratio = pca.explained_variance_ratio_\nnum_components = len(explained_variance_ratio)\ncomponents = list(range(1, num_components + 1))\n\nfig = go.Figure(data=[go.Bar(x=components, y=explained_variance_ratio)])\n\nfig.update_layout(\n    title=\"Proportion of Variance Explained by Principal Components\",\n    xaxis_title=\"Principal Components\",\n    yaxis_title=\"Explained Variance Ratio\",\n    yaxis=dict(tickformat=\".2%\"),\n)\n\nfig.show()\n```\n\n```{python}\nloadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i}' for i in range(1, num_components+1)], index=df_raw.columns[3:]) \n\nPC1 = loadings.reindex(loadings['PC1'].abs().sort_values(ascending=False).index)['PC1'].head()\n\nPC2 = loadings.reindex(loadings['PC2'].abs().sort_values(ascending=False).index)['PC2'].head()\n\nPC3 = loadings.reindex(loadings['PC3'].abs().sort_values(ascending=False).index)['PC3'].head()\n\nPC4 = loadings.reindex(loadings['PC4'].abs().sort_values(ascending=False).index)['PC4'].head()\n\nPC5 = loadings.reindex(loadings['PC5'].abs().sort_values(ascending=False).index)['PC5'].head()\n\nPC6 = loadings.reindex(loadings['PC6'].abs().sort_values(ascending=False).index)['PC6'].head()\n\n```\n\n## PC1\n\nThe first principal component seems to be some measure of traditional metrics of the society including how White the population is, both religious and nonreligious organizations, low criminality, and percent married.\n\n```{python}\nPC1\n```\n\n## PC2\n\nThe second principal component seems to be some measure of how economically disadvantaged the population is, with measures like poverty rate highly correlated and median household income negatively correlated. Interestingly, religious congregations and teen births per 1k is also highly correlated with this principal component. It seems like this principal component has some aspects of traditionality of the society, but the negative aspects of it.\n\n```{python}\nPC2\n```\n\n## PC3\n\nThe third principal component seems to be some measure of how busy the area is. It is highly correlated with the percent of the population that is employed in the management, production, and construction industry.\n\n```{python}\nPC3\n```\n\n## PC4\n\nThe fourth principal component seems to be some negative measure of civic engagement and social capital.\n\n```{python}\nPC4\n```\n\n## PC5\n\nNot exactly sure what this principal component is measuring.\n\n```{python}\nPC5\n```\n\n## PC6\n\nProbably measures a variable related to something with race and crime\n\n```{python}\nPC6\n```","srcMarkdownNoYaml":"\n\nPrincipal Component Analysis (PCA) is a statistical technique used to simplify complex data by reducing the number of variables while retaining most of the original information. In other words, it helps to identify the most important patterns or relationships in large datasets by transforming them into a set of linearly uncorrelated variables, known as principal components.\n\nPCA works by identifying the underlying structure of the data and extracting the directions of maximum variance, which are the principal components. Each principal component is a linear combination of the original variables, and they are orthogonal to each other, meaning that they are uncorrelated. The first principal component captures the most significant variation in the data, and subsequent components capture decreasing amounts of variation. By selecting the appropriate number of principal components, we can reduce the dimensionality of the dataset while retaining the essential information.\n\nIn this data science project, I used Principal Component Analysis (PCA) to visualize and explore the similarity of various counties in the United States based on a set of demographic metrics including th following:\n\n-   Population estimates\n-   Poverty estimates\n-   Unemployment rates\n-   Crime rates\n-   Racial demographics\n-   Political leanings\n-   Job industry composition\n-   General health status\n-   Social capital\n\nThe primary utility of using PCA in this context is to reduce the dimensionality of the dataset while retaining as much information as possible. By doing so, we can efficiently examine patterns and relationships among counties in a lower-dimensional space.\n\nFor each of these data, I applied PCA to transform the original high-dimensional data into a lower-dimensional space that still captures most of the variation present in the original dataset.\n\nBy plotting the first two or three principal components on a 2D scatter plot, we can visualize the relationships among counties based on their transformed coordinates. This allows us to identify clusters of similar counties, outliers, or any other interesting patterns. In the code, I also highlighted a specific county (represented by highlighted_area), making it easier to identify its position relative to other counties in the reduced-dimensional space.\n\nAdditionally, I created a bar chart displaying the proportion of variance explained by each principal component, which helps to determine the optimal number of components to retain for further analysis or modeling.\n\nThe use of PCA in this project allows us to effectively summarize the complex, high-dimensional relationships between counties, making it easier to identify patterns and interpret the data.\n\n```{python}\n# Import\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\nimport plotly.graph_objects as go\n```\n\n# Population\n\n```{python}\n# Population Estimates\npop = pd.read_excel(\"PopulationEstimates.xlsx\", skiprows=4)\n\npop.columns = ['fips_code', 'state', 'area_name', 'rucc_2013', 'pop_1990', 'pop_2000', 'pop_2010', 'pop_2020', 'pop_2021']\n\npop = (pop\n    .filter(['fips_code', 'state', 'area_name', 'rucc_2013','pop_2021'], axis=1)\n    .query(\"state != 'Puerto Rico'\"))\n\npol = pd.read_csv('politics.csv').rename({'fips':'fips_code'}, axis=1)\n\npop.head()\n```\n\n```{python}\ndef make_pca(new_df, highlighted_area, dim=2):\n    # Merge\n    df_raw = (pd\n        .merge(pop, new_df, on='fips_code', how='left')\n        .dropna()\n        .query(\"state != 'Puerto Rico'\"))\n\n    # Normalize data\n    scaler = StandardScaler()\n    df = scaler.fit_transform(df_raw.drop(['fips_code', 'state', 'area_name'], axis=1))\n    df_name = df_raw[['fips_code', 'state', 'area_name']].reset_index(drop=True)\n\n    # PCA analysis\n    pca = PCA()\n    pca.fit(df)\n    pcs = pca.transform(df)\n    pcs_df = pd.DataFrame(pcs)\n    df_final = pd.concat([df_name, pcs_df], axis=1)\n\n    df_return = df_final.drop(['state', 'area_name'], axis=1)\n\n    fig2 = pca_variance(pca)\n\n    # Marks\n    marker_colors = df_final['fips_code'].apply(lambda x: 'red' if x == highlighted_area else 'blue')\n    marker_sizes = df_final['fips_code'].apply(lambda x: 15 if x == highlighted_area else 5)\n\n    if dim == 2:\n        # Chart PC distance 2D\n        fig = px.scatter(df_final,\n            x=df_final.columns[len(df_name.columns)],\n            y=df_final.columns[len(df_name.columns) + 1],\n            hover_data=['fips_code','state','area_name'],\n            custom_data=['fips_code'],\n            color=marker_colors,\n            size=marker_sizes,\n            color_discrete_map={'red': 'red', 'blue': 'blue'})\n\n        fig.update_traces(marker=dict(symbol='circle', line=dict(width=1))\n        )\n\n        return fig, fig2, df_return\n\n    elif dim == 3:\n        fig = px.scatter_3d(df_final,\n            x=df_final.columns[len(df_name.columns)],\n            y=df_final.columns[len(df_name.columns) + 1],\n            z=df_final.columns[len(df_name.columns) + 2],\n            hover_data=['fips_code','state','area_name'],\n            custom_data=['fips_code'],\n            color=marker_colors,\n            size=marker_sizes,\n            color_discrete_map={'red': 'red', 'blue': 'blue'})\n\n        fig.update_traces(marker=dict(symbol='circle', line=dict(width=1)))\n\n        return fig, fig2, df_return\n\ndef pca_variance(pca):\n    # Explained variance\n    explained_variance_ratio = pca.explained_variance_ratio_\n    num_components = len(explained_variance_ratio)\n    components = list(range(1, num_components + 1))\n\n    fig = go.Figure(data=[go.Bar(x=components, y=explained_variance_ratio)])\n\n    fig.update_layout(\n        title=\"Proportion of Variance Explained by Principal Components\",\n        xaxis_title=\"Principal Components\",\n        yaxis_title=\"Explained Variance Ratio\",\n        yaxis=dict(tickformat=\".2%\"),\n    )\n\n    return fig\n\n```\n\n# Poverty\n\nPoverty variables:\n\n-   percent of population below poverty level overall in county 2020 (pct_povall_2020)\n-   percent of population below poverty level for minors in county 2020 (pct_pov017_2020)\n-   median household income in county 2020 (medhhinc_2020)\n\n```{python}\n# Poverty Estimates\npov = pd.read_excel(\"PovertyEstimates.xlsx\", skiprows=4)\n\npov.columns = ['fips_code', 'stabr', 'area_name', 'rucc_2003', 'uic_2003', 'rucc_2013', 'uic_2013', 'povall_2020', 'ci90lb_all_2020', 'ci90ub_all_2020', 'pct_povall_2020', 'ci90lb_allp_2020', 'ci90ub_allp_2020', 'pov017_2020', 'ci90lb_017_2020', 'ci90ub_017_2020', 'pct_pov017_2020', 'ci90lb_017p_2020', 'ci90ub_017p_2020', 'pov517_2020', 'ci90lb_517_2020', 'ci90ub_517_2020', 'pct_pov517_2020', 'ci90lb_517p_2020', 'ci90ub_517p_2020', 'medhhinc_2020', 'ci90lb_inc_2020', 'ci90ub_inc_2020', 'pov04_2020', 'ci90lb_04_2020', 'ci90ub_04_2020', 'pct_pov04_2020', 'ci90lb_04p_2020', 'ci90ub_04p_2020']\n\npov_filtered_columns = ['fips_code','pct_povall_2020','pct_pov017_2020','medhhinc_2020']\n\npov = pov[pov_filtered_columns]\n```\n\n# Education\n\nEducation variables:\n\n-   percent of population with less than high school diploma in county 2017-2021 (pct_less_than_hs_2017_21)\n-   percent of population with high school diploma in county 2017-2021 (pct_hs_diploma_2017_21)\n-   percent of population with some college or associate degree in county 2017-2021 (pct_some_college_assoc_degree_2017_21)\n-   percent of population with bachelor's degree or higher in county 2017-2021 (pct_bachelors_plus_2017_21)\n\n```{python}\n# Education\nedu = pd.read_excel(\"Education.xlsx\", skiprows=3)\n\nedu.columns = ['fips_code', 'state', 'area_name', 'rucc_2003', 'uic_2003', 'rucc_2013', 'uic_2013', 'less_than_hs_1970', 'hs_diploma_1970', 'some_college_1_3_yrs_1970', 'college_4_yrs_plus_1970', 'pct_less_than_hs_1970', 'pct_hs_diploma_1970', 'pct_some_college_1_3_yrs_1970', 'pct_college_4_yrs_plus_1970', 'less_than_hs_1980', 'hs_diploma_1980', 'some_college_1_3_yrs_1980', 'college_4_yrs_plus_1980', 'pct_less_than_hs_1980', 'pct_hs_diploma_1980', 'pct_some_college_1_3_yrs_1980', 'pct_college_4_yrs_plus_1980', 'less_than_hs_1990', 'hs_diploma_1990', 'some_college_assoc_degree_1990', 'bachelors_plus_1990', 'pct_less_than_hs_1990', 'pct_hs_diploma_1990', 'pct_some_college_assoc_degree_1990', 'pct_bachelors_plus_1990', 'less_than_hs_2000', 'hs_diploma_2000', 'some_college_assoc_degree_2000', 'bachelors_plus_2000', 'pct_less_than_hs_2000', 'pct_hs_diploma_2000', 'pct_some_college_assoc_degree_2000', 'pct_bachelors_plus_2000', 'less_than_hs_2008_12', 'hs_diploma_2008_12', 'some_college_assoc_degree_2008_12', 'bachelors_plus_2008_12', 'pct_less_than_hs_2008_12', 'pct_hs_diploma_2008_12', 'pct_some_college_assoc_degree_2008_12', 'pct_bachelors_plus_2008_12', 'less_than_hs_2017_21', 'hs_diploma_2017_21', 'some_college_assoc_degree_2017_21', 'bachelors_plus_2017_21', 'pct_less_than_hs_2017_21', 'pct_hs_diploma_2017_21', 'pct_some_college_assoc_degree_2017_21', 'pct_bachelors_plus_2017_21']\n\nedu_filtered_columns = [col for col in edu.columns if ('2017' in col and 'pct' in col) or 'fips' in col.lower()]\n\nedu = edu[edu_filtered_columns]\n```\n\n# Unemployment\n\nUnemployment variables:\n\n-   Civilian labor force in county 2021 (civ_labor_force_2021)\n-   Employed in county 2021 (employed_2021)\n-   Unemployed in county 2021 (unemployed_2021)\n-   Unemployment rate in county 2021 (unemp_rate_2021)\n-   Gini coefficient in county 2019? (Gini.Coefficient)\n\n```{python}\n# Unemployment\nunemp = pd.read_excel(\"Unemployment.xlsx\", skiprows=4)\n\nunemp.columns = ['fips_code', 'state', 'area_name', 'rucc_2013', 'uic_2013', 'metro_2013', 'civ_labor_force_2000', 'employed_2000', 'unemployed_2000', 'unemp_rate_2000', 'civ_labor_force_2001', 'employed_2001', 'unemployed_2001', 'unemp_rate_2001', 'civ_labor_force_2002', 'employed_2002', 'unemployed_2002', 'unemp_rate_2002', 'civ_labor_force_2003', 'employed_2003', 'unemployed_2003', 'unemp_rate_2003', 'civ_labor_force_2004', 'employed_2004', 'unemployed_2004', 'unemp_rate_2004', 'civ_labor_force_2005', 'employed_2005', 'unemployed_2005', 'unemp_rate_2005', 'civ_labor_force_2006', 'employed_2006', 'unemployed_2006', 'unemp_rate_2006', 'civ_labor_force_2007', 'employed_2007', 'unemployed_2007', 'unemp_rate_2007', 'civ_labor_force_2008', 'employed_2008', 'unemployed_2008', 'unemp_rate_2008', 'civ_labor_force_2009', 'employed_2009', 'unemployed_2009', 'unemp_rate_2009', 'civ_labor_force_2010', 'employed_2010', 'unemployed_2010', 'unemp_rate_2010', 'civ_labor_force_2011', 'employed_2011', 'unemployed_2011', 'unemp_rate_2011', 'civ_labor_force_2012', 'employed_2012', 'unemployed_2012', 'unemp_rate_2012', 'civ_labor_force_2013', 'employed_2013', 'unemployed_2013', 'unemp_rate_2013', 'civ_labor_force_2014', 'employed_2014', 'unemployed_2014', 'unemp_rate_2014', 'civ_labor_force_2015', 'employed_2015', 'unemployed_2015', 'unemp_rate_2015', 'civ_labor_force_2016', 'employed_2016', 'unemployed_2016', 'unemp_rate_2016', 'civ_labor_force_2017', 'employed_2017', 'unemployed_2017', 'unemp_rate_2017', 'civ_labor_force_2018', 'employed_2018', 'unemployed_2018', 'unemp_rate_2018', 'civ_labor_force_2019', 'employed_2019', 'unemployed_2019', 'unemp_rate_2019', 'civ_labor_force_2020', 'employed_2020', 'unemployed_2020', 'unemp_rate_2020', 'civ_labor_force_2021', 'employed_2021', 'unemployed_2021', 'unemp_rate_2021', 'med_hh_income_2020', 'med_hh_income_pct_state_total_2020']\n\nunemp_filtered_columns = [col for col in unemp.columns if '2021' in col or 'fips' in col.lower()]\n\nunemp = unemp[unemp_filtered_columns]\n\nunemp_cols = ['civ_labor_force_2021','employed_2021','unemployed_2021']\n\nunemp2 = pd.merge(unemp, pop.filter(['fips_code','pop_2021']), how = 'left',\n     left_on='fips_code', right_on='fips_code'\n)\n\nunemp[unemp_cols] = (unemp2[unemp_cols].div(unemp2['pop_2021'], axis=0))*100\n\nunemp = pd.merge(unemp, pol[['fips_code','Gini.Coefficient']], how = 'left',\n     left_on='fips_code', right_on='fips_code'\n)\n```\n\n# Crime\n\nCrime Variables\n\n-   Crime rate/100k people\n-   Murders/capita\n-   Rape/capita\n-   Robbery/capita\n-   Aggravated assault/capita\n-   Burglaries/capita\n-   Larcenies/capita\n-   Motor vehicle thefts/capita\n-   Arsons/capita\n\n```{python}\n# Crime\ncrime = pd.read_csv(\"crime_clean.csv\").drop('Unnamed: 0', axis=1)\n\ncrime.columns = ['fips_code','crime_rate_per_100k','murder','rape','robbery','ag_assault','burglry','larceny','mv_theft','arson','population']\n\ncrime_cols = ['murder','rape','robbery','ag_assault','burglry','larceny','mv_theft','arson']\n\ncrime[crime_cols] = crime[crime_cols].div(crime['population'], axis=0)*100000\n\ncrime = crime.drop('population', axis=1)\na, b, crime_df = make_pca(crime, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Race\n\nRace Variables\n\n-   White (%)\n-   Black (%)\n-   American Indian/Alaska Native (%)\n-   Asian (%)\n-   Native Hawaiian/Pacific Islander (%)\n-   Other race (%)\n-   Two or more races (%)\n-   Hispanic/Latino (%)\n\n```{python}\n# Race\nrace = pd.read_csv('race.csv')\n\nrace_cols = ['white', 'black','amerindian', 'asian', 'hawaiian_or_PI', 'other_race', 'two_or_more','hispanic']\n\nrace[race_cols] = race.filter(race_cols, axis=1).div(race['total_population'],axis=0)*100\n\nrace = race.drop(['county_name','total_population'], axis=1)\na, b, race_df = make_pca(race, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Politics\n\nPolitical Variables\n\n-   Republican Vote Share in 2016 Presidential Election (%)\n-   Democratic Vote Share in 2016 Presidential Election (%)\n-   Libertarian Vote Share in 2016 Presidential Election (%)\n\n```{python}\n# Politics\npolitics = ['fips_code','rep16_frac',\n'dem16_frac',\n'libert16_frac']\npolitics = pol[politics].fillna(0)\n```\n\n# Job Industry\n\nJob Industry Variables\n\n-   Management, professional, and related occupations (%)\n-   Service occupations (%)\n-   Sales and office occupations (%)\n-   Farming, fishing, and forestry occupations (%)\n-   Construction, extraction, maintenance, and repair occupations (%)\n-   Production, transportation, and material moving occupations (%)\n\n```{python}\n# Job Industry\njobs = ['fips_code','Management.professional.and.related.occupations',\n'Service.occupations',\n'Sales.and.office.occupations',\n'Farming.fishing.and.forestry.occupations',\n'Construction.extraction.maintenance.and.repair.occupations',\n'Production.transportation.and.material.moving.occupations']\n\njobs = pol[jobs].fillna(0)\n\njobs.columns = ['fips_code', 'mgmt', 'service', 'sales', 'farming', 'construction', 'production']\n\njobs2 = pd.merge(jobs, pop.filter(['fips_code','pop_2021'], axis=1), how = 'left', left_on='fips_code', right_on='fips_code')\n\njobs_cols = ['mgmt', 'service', 'sales', 'farming', 'construction', 'production']\n\njobs[jobs_cols] = ((jobs2[jobs_cols].div(jobs2['pop_2021'], axis=0))*100000).fillna(0)\na, b, jobs_df= make_pca(jobs, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Health\n\nHealth Variables\n\n-   Poor physical health days (%)\n-   Poor mental health days (%)\n-   Low birthweight (%)\n-   Teen births (%)\n-   Adult smoking (%)\n-   Adult obesity (%)\n-   Diabetes (%)\n-   Sexually transmitted infections (%)\n\n```{python}\n# Health\nhealth = ['fips_code','Poor.physical.health.days', 'Poor.mental.health.days', 'Low.birthweight', 'Teen.births', 'Adult.smoking', 'Adult.obesity', 'Diabetes','Sexually.transmitted.infections']\n\nhealth = pol[health].fillna(0)\n\nhealth.columns = ['fips_code', 'poor_phys', 'poor_mental', 'low_birth_lbs', 'teen_births', 'smokers','obesity','diabetes', 'stds']\n\nhealth2 = pd.merge(health, pop.filter(['fips_code','pop_2021'], axis=1), how = 'left', left_on='fips_code', right_on='fips_code')\n\nhealth_cols = ['poor_phys', 'poor_mental', 'low_birth_lbs', 'teen_births', 'smokers','obesity','diabetes', 'stds']\n\nhealth[health_cols] = ((health[health_cols].div(jobs2['pop_2021'], axis=0))*100000).fillna(0)\na, b, health_df = make_pca(health, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# Social Capital\n\n-   Percentage of births to unmarried mothers (%)\n-   Percentage of women who are married (%)\n-   Percentage of children in single-parent households (%)\n-   Non-religious non-profit organizations per 1,000 people\n-   Religious congregations per 1,000 people\n-   Informal civic engagement sub-index\n-   Presidential election voting rate 2012-2016\n-   Mail-back census response rate\n-   Confidence in institutions sub-index\n-   Membership organizations per 1,000 people\n-   Recreation and leisure establishments per 1,000 people\n-   Associations per 1,000 people (Penn State method)\n-   Non-religious and religious organizations per 1,000 people\n-   Religious adherents per 1,000 people\n-   Percentage of people who receive emotional support sometimes, rarely, or never\n-   Share of middle-class itemizers deducting charitable contributions (%)\n-   Ratio of 80th to 20th percentile household income\\*\n\n```{python}\n# Social Capital\nsoccap = pd.read_csv('soccap.csv')\n\nsoccap.columns=['fips_code', 'pct_births_unmarried', 'pct_women_married', 'pct_children_single_parent', 'non_religious_non_profit_orgs_per_1k',\n'religious_congregations_per_1k', 'informal_civic_engagement_subidx', 'presidential_election_voting_rate_12_16',\n'mail_back_census_response_rate', 'confidence_in_institutions_subidx', 'membership_orgs_per_1k', 'recreation_leisure_est_per_1k',\n'associations_per_1k_penn_state_method', 'non_religious_and_religious_orgs_per_1k', 'religious_adherents_per_1k',\n'pct_emotional_support_sometimes_rarely_or_never', 'charitable_contributions_share_of_agi_middle_class_itemizers',\n'share_of_middle_class_itemizers_deducting_charitable_contributions', 'ratio_80th_to_20th_pct_hh_income']\na, b, soccap_df = make_pca(soccap, 48209, dim=2)\na\n```\n\n```{python}\nb\n```\n\n# PCA with all the data\n\n```{python}\ndf_raw = pop.drop(['rucc_2013'], axis = 1).pipe(\n    pd.merge, pov, on='fips_code').pipe(\n    pd.merge, edu, on='fips_code').pipe(\n    pd.merge, unemp, on='fips_code').pipe(\n    pd.merge, crime, on='fips_code').pipe(\n    pd.merge, race, on='fips_code').pipe(\n    pd.merge, politics, on='fips_code').pipe(\n    pd.merge, jobs, on='fips_code').pipe(\n    pd.merge, health, on='fips_code').pipe(\n    pd.merge, soccap, on='fips_code')\n\nfor col in df_raw.drop(['fips_code','state','area_name'],axis=1).columns:\n    df_raw[col] = df_raw[col].fillna(df_raw[col].mean())\n\nhighlighted_area = 48209\n\n# Normalize data\nscaler = StandardScaler()\ndf = scaler.fit_transform(df_raw.drop(['fips_code', 'state', 'area_name'], axis=1))\ndf_name = df_raw[['fips_code', 'state', 'area_name']].reset_index(drop=True)\n\n# PCA analysis\npca = PCA()\npca.fit(df)\npcs = pca.transform(df)\npcs_df = pd.DataFrame(pcs)\ndf_final = pd.concat([df_name, pcs_df], axis=1)\n\nfig2 = pca_variance(pca)\n\n# Marks\nmarker_colors = df_final['fips_code'].apply(lambda x: 'red' if x == highlighted_area else 'blue')\nmarker_sizes = df_final['fips_code'].apply(lambda x: 15 if x == highlighted_area else 5)\n\n\n# Chart PC distance 2D\nfig = px.scatter(df_final,\n    x=df_final.columns[len(df_name.columns)],\n    y=df_final.columns[len(df_name.columns) + 1],\n    hover_data=['fips_code','state','area_name'],\n    custom_data=['fips_code'],\n    color=marker_colors,\n    size=marker_sizes,\n    color_discrete_map={'red': 'red', 'blue': 'blue'})\n\nfig.update_traces(marker=dict(symbol='circle', line=dict(width=1))\n)\n\nfig.show()\n```\n\n```{python}\nfig = px.scatter_3d(df_final,\n    x=df_final.columns[len(df_name.columns)],\n    y=df_final.columns[len(df_name.columns) + 1],\n    z=df_final.columns[len(df_name.columns) + 2],\n    hover_data=['fips_code','state','area_name'],\n    custom_data=['fips_code'],\n    color=marker_colors,\n    size=marker_sizes,\n    color_discrete_map={'red': 'red', 'blue': 'blue'})\n\nfig.update_traces(marker=dict(symbol='circle', line=dict(width=1)))\n\nfig.show()\n```\n\n```{python}\n# Explained variance\nexplained_variance_ratio = pca.explained_variance_ratio_\nnum_components = len(explained_variance_ratio)\ncomponents = list(range(1, num_components + 1))\n\nfig = go.Figure(data=[go.Bar(x=components, y=explained_variance_ratio)])\n\nfig.update_layout(\n    title=\"Proportion of Variance Explained by Principal Components\",\n    xaxis_title=\"Principal Components\",\n    yaxis_title=\"Explained Variance Ratio\",\n    yaxis=dict(tickformat=\".2%\"),\n)\n\nfig.show()\n```\n\n```{python}\nloadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i}' for i in range(1, num_components+1)], index=df_raw.columns[3:]) \n\nPC1 = loadings.reindex(loadings['PC1'].abs().sort_values(ascending=False).index)['PC1'].head()\n\nPC2 = loadings.reindex(loadings['PC2'].abs().sort_values(ascending=False).index)['PC2'].head()\n\nPC3 = loadings.reindex(loadings['PC3'].abs().sort_values(ascending=False).index)['PC3'].head()\n\nPC4 = loadings.reindex(loadings['PC4'].abs().sort_values(ascending=False).index)['PC4'].head()\n\nPC5 = loadings.reindex(loadings['PC5'].abs().sort_values(ascending=False).index)['PC5'].head()\n\nPC6 = loadings.reindex(loadings['PC6'].abs().sort_values(ascending=False).index)['PC6'].head()\n\n```\n\n## PC1\n\nThe first principal component seems to be some measure of traditional metrics of the society including how White the population is, both religious and nonreligious organizations, low criminality, and percent married.\n\n```{python}\nPC1\n```\n\n## PC2\n\nThe second principal component seems to be some measure of how economically disadvantaged the population is, with measures like poverty rate highly correlated and median household income negatively correlated. Interestingly, religious congregations and teen births per 1k is also highly correlated with this principal component. It seems like this principal component has some aspects of traditionality of the society, but the negative aspects of it.\n\n```{python}\nPC2\n```\n\n## PC3\n\nThe third principal component seems to be some measure of how busy the area is. It is highly correlated with the percent of the population that is employed in the management, production, and construction industry.\n\n```{python}\nPC3\n```\n\n## PC4\n\nThe fourth principal component seems to be some negative measure of civic engagement and social capital.\n\n```{python}\nPC4\n```\n\n## PC5\n\nNot exactly sure what this principal component is measuring.\n\n```{python}\nPC5\n```\n\n## PC6\n\nProbably measures a variable related to something with race and crime\n\n```{python}\nPC6\n```"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"toc":false,"output-file":"pca.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.330","max-width":"300%","theme":"cosmo","title-block-banner":true,"title":"A county like mine","categories":["politics","visualization","statistics","python"],"image":"pca.png","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}